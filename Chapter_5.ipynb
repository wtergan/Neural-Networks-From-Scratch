{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Network Error with Loss\n",
    "\n",
    "In chapter two, our model is currently random. We need a way to calculate how wrong our neural network is at current prediction and begin adjusting the weights and biases to decrease error over time.\n",
    "\n",
    "To quantify how wrong our model is , we define it as the ***loss function***\n",
    "\n",
    "***loss function***\n",
    "\n",
    "> also referred to as the cost function, it quantifies how wrong the model is. We ideally want this loss to be 0.\n",
    "\n",
    "note that argmax applied to the output gives us the index of the biggest value in the softmax output. This index indicates the value with the biggest confidence. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Categorical Cross Entropy Loss*\n",
    "\n",
    ">  used to compare a 'ground-truth' probability (y) and some predicted distribution (y-hat or predictions). \n",
    "\n",
    "> one of the most commonly used loss functions with a softmax activation on the output layer.\n",
    "\n",
    "$ L = - \\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{i,j} \\log(p_{i,j}) $,\n",
    "\n",
    "where:\n",
    "\n",
    "* $ N $ is the number of samples\n",
    "* $ M $ is the number of classes\n",
    "* $ y $ is the true label, a one-hot encoded vector of size $ M $\n",
    "* $ p $ is the predict label, a probabailiy distribution over the $ M $ classes\n",
    "* $ j $ is to index the classes.\n",
    "\n",
    "also denoted as:\n",
    "\n",
    "$ L = -  \\sum_{j} y_{i,j} \\log(p_{i,j}) $\n",
    "\n",
    "where:\n",
    "\n",
    "* $ L $ denotes the sample loss value\n",
    "* $ i $ is the i-th sample in the set\n",
    "* $ j $ is the label/output index\n",
    "* $ y $ denotes the target values\n",
    "* $ p $ denotes the predicted values\n",
    "\n",
    "which then is simplified further to:\n",
    "\n",
    "$ L = - \\log(p_{i,k}) $ , where $ k $ is an index of the 'true' probability\n",
    "\n",
    "we compare the output probability distribution (predict) with the one-hot vector probability distribution (true, or ground truth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "#an example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "#ground truth, this is a one-hot vector\n",
    "target_output = [1,0,0]\n",
    "\n",
    "loss = - (target_output[0]*math.log(softmax_output[0]) + target_output[1]*math.log(softmax_output[1]) + target_output[2]*math.log(softmax_output[2]))\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this can be simplified to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = -(math.log(softmax_output[0]))\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember:\n",
    "\n",
    "> the loss value raises the confidence level, approaching 0.\n",
    "\n",
    "but what is this 'log' that we are using?\n",
    "\n",
    "> ***logarithm***, function that determines the power to which a given number (the base) must be raised to produce a vertain value\n",
    "\n",
    "$ a^{x} = b $, to find x, we use $ log $ , so $ log_{a}b = x $\n",
    "\n",
    "> ***natural logarithm***, also referred to just the log, is where $ e $ is the base, so that:\n",
    "\n",
    "$ e^{x} = b, log_{e}b = x $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "this value is x, so that when e is exponentiated with this value, result is b.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = 5.2\n",
    "x = np.log(b)\n",
    "print(x)\n",
    "print('this value is x, so that when e is exponentiated with this value, result is b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2\n",
      "this value is b!\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(x))\n",
    "print('this value is b!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets consider a neural network that performs the classification between 3 classes, and the network classifies in batches of 3, so that the output layer yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7  0.1  0.2 ]\n",
      " [0.1  0.5  0.4 ]\n",
      " [0.02 0.9  0.08]] shape:  (3, 3)\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "print(softmax_outputs, 'shape: ', softmax_outputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets consider our target values (the one hot vector of true labels), where:\n",
    "\n",
    "* dog is class 0 (index 0)\n",
    "* cat is class 1 (index 1)\n",
    "* human is class 2 (index 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_targets = [0, 1, 1] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this class_targets represents the true label of each input in the sample, so:\n",
    "\n",
    ">for first image: target value is 0, which denotes a dog\n",
    "\n",
    "> for second image: target value is 1, which denotes a cat\n",
    "\n",
    "> for third image: target value is 1, which denotes a cat\n",
    "\n",
    "if the third index was 2, that would mean that the target value that maps to the third image would be a human.\n",
    "\n",
    "Note:\n",
    "\n",
    "* the target values are specific to each input sample, rather than all of the input samples.\n",
    "\n",
    "with the collection fo softmax outputs and their indended targets, we can map these indices to retrieve the values from the softmax distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets= [0,1,1]\n",
    "\n",
    "for target_index, distributions in zip(class_targets, softmax_outputs):\n",
    "    print(distributions[target_index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what the above code does is go through both the softmax array and the class target vector.\n",
    "\n",
    "outputs the confidence score in index 0 for first sample,\n",
    "\n",
    "outputs the confidence score in index 1 for the second sample,\n",
    " \n",
    "outputs the confidence score in index 1 for the third sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can simplify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]])\n",
    "class_targets= [0,1,1]\n",
    "\n",
    "print(softmax_outputs[[0,1,2], class_targets])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code is printing the specific elements of the softmax output that correspond to the target classes\n",
    "\n",
    "> range creates a list of integers from 0 to the number of input samples-1\n",
    "\n",
    "> in the first index, the range(len(softmax_outputs)) is used to select the rows of the array that correspond to each input sample\n",
    "\n",
    "> in the second index, class_targets is used to select the columns of the array that correspond to the target classes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets now apply the -log to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n",
      "This result is the loss for each of the samples in the batch\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets]))\n",
    "print('This result is the loss for each of the samples in the batch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice how for the third sample, the loss is ~0.1, the lower the loss is better, and this is seen based off of the probability distribution for the third sample. the confidence score of 0.9 is the best out of the three. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, lets compute the arithmetic mean to find the average loss per batch so that we can have an idea about how our model is doing during the training\n",
    "\n",
    "$ sum(iterable) / len(iterable) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the list of losses for each sample in the batch:  [0.35667494 0.69314718 0.10536052]\n",
      "this is the average loss of the batch:  0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -(np.log(softmax_outputs[range(len(softmax_outputs)), class_targets]))\n",
    "print('this is the list of losses for each sample in the batch: ', neg_log)\n",
    "\n",
    "average_loss = sum(neg_log) / len(neg_log)\n",
    "print('this is the average loss of the batch: ', average_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sometimes, we have to check if the targets are one-hot encoded. \n",
    "\n",
    "> if the targets have only one dimension, like a list, or like the one in the code above, this means that the targets are ***sparse*** \n",
    "\n",
    "* we can write down the number that represents the correct class\n",
    "\n",
    "> if it is more than one dimension, then it may be one-hot encoded. \n",
    "\n",
    "if the targets are one-hot encoded, we need to multiply the confidences by the targets, and then zero out all of the values except for the correct labels. Then we add up the numbers along the row axis (axis 1), so that we can calculate the loss for the one-hot encoded targets.\n",
    "\n",
    "add a test in the code to check the number of dimensions of the targets, move calculations of the log values outside of the new if statement, and implement a solution for the one-hot encoded targets following the first equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3) 2\n",
      "the length of the shape of the target values is n is nD array\n",
      "this is the confidence scores [0.7 0.5 0.9]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1, 0, 0],\n",
    "                        [0, 1, 0],\n",
    "                        [0, 1, 0]])\n",
    "\n",
    "print(class_targets.shape, len(class_targets.shape)) \n",
    "print('the length of the shape of the target values is n is nD array')                       \n",
    "\n",
    "'''probabilities for target values only if categorical labels\n",
    "   if target values is simply a one-hot vector (1D), simply output the confidences \n",
    "    that maps to the target values, but if the target values is 2D, multiply the \n",
    "    confidences with the target values along the columns, and sum them up '''\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(softmax_outputs * class_targets, axis=1)\n",
    "\n",
    "print('this is the confidence scores', correct_confidences) \n",
    "\n",
    "'''losses'''\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! however, there is one more problem we have to solve.\n",
    "\n",
    "the softmax output onsists of numbers in the range from 0 to 1. its possible that the model will have the full confidence for one label making all of the remaining confidences zero. its also possible that the model will assign full confidenc3s to a value that wasn't the target.\n",
    "\n",
    "if we then try to calculate the loss of this confidence of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/b0xs89p96wg8lzcfm69hdsm40000gn/T/ipykernel_4636/3467695962.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  -np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "-np.log(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ log(0) $ is undefined, since if $ y = log(x) $, then $ e^{y} = x $, \n",
    "\n",
    "a constant e to any power is always a positive number, and there is no y resulting in $ e^{y} = 0 $"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "this value prevents the loss from being exactly 0, making it a very small value instead, but it wont make it a negative value and won't biases overall loss towards 1.\n",
    "\n",
    "this method can perform clipping on an array of values, so we can apply it ot the predictions directly and save this as a separate array\n",
    "\n",
    "> this changes the values of the predictions rray (y_pred) to be a range between 1e-7 and 1 - 1e-7, not 0 and 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy Loss Class\n",
    "\n",
    "calculating the overall loss, no matter which loss function we use, will ALWAYS be the same, which is the arithmetic mean of all of the sample losses.\n",
    "\n",
    "lets create a class that will calculate this mean value from the returned sample losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common loss class\n",
    "class Loss:\n",
    "    #calculates the data and regularization losses. Given model input and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets show how this loss class will be implemented if we used the categorical cross entropy loss from above\n",
    "\n",
    "this class inherits the Loss class and performs all the error calculations that we derived throughout the chapter and can be used as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "#Cross Entropy loss\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        #number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        #clip data to prevent division by 0. clip both sides to not drag the mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        #probabilities for target values, only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        #losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods  \n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)              "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Code Up To This Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is first 5 samples in the dataset: \n",
      " [[0.         0.        ]\n",
      " [0.00299556 0.00964661]\n",
      " [0.01288097 0.01556285]\n",
      " [0.02997479 0.0044481 ]\n",
      " [0.03931246 0.00932828]] shape:  (300, 2)\n",
      "this is the first 5 values in the y: \n",
      " [0 0 0 0 0] shape:  (300,)\n",
      "output values of the first layer: \n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.0475188e-04  1.1395361e-04 -4.7983500e-05]\n",
      " [-2.7414842e-04  3.1729150e-04 -8.6921798e-05]\n",
      " [-4.2188365e-04  5.2666257e-04 -5.5912682e-05]\n",
      " [-5.7707680e-04  7.1401405e-04 -8.9430439e-05]] shape:  (300, 3)\n",
      "output values after sent to the reLU activation function: \n",
      " [[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n",
      "output values of the second layer: \n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.8183968e-07 -1.5235776e-07  1.2281279e-06]\n",
      " [-5.0631292e-07 -4.2422371e-07  3.4195891e-06]\n",
      " [-8.4041352e-07 -7.0415609e-07  5.6760728e-06]\n",
      " [-1.1393766e-06 -9.5464793e-07  7.6952419e-06]]\n",
      "output values after sent to the softmax activation function: \n",
      " [[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]\n",
      " [0.33333284 0.33333287 0.3333343 ]\n",
      " [0.33333182 0.3333319  0.33333626]\n",
      " [0.33333182 0.3333319  0.33333623]\n",
      " [0.3333315  0.3333316  0.33333692]\n",
      " [0.33333105 0.3333312  0.33333772]\n",
      " [0.3333308  0.33333096 0.3333382 ]\n",
      " [0.3333311  0.33333126 0.33333766]\n",
      " [0.33333054 0.3333307  0.33333874]\n",
      " [0.3333301  0.3333303  0.3333396 ]\n",
      " [0.33333012 0.33333033 0.33333954]\n",
      " [0.33332998 0.33333018 0.3333398 ]\n",
      " [0.3333313  0.33333188 0.33333686]\n",
      " [0.3333294  0.33332965 0.33334094]\n",
      " [0.33332995 0.33333033 0.33333975]\n",
      " [0.33332875 0.33332902 0.33334222]\n",
      " [0.33332875 0.33332902 0.3333422 ]\n",
      " [0.33333093 0.33333182 0.3333372 ]\n",
      " [0.33333173 0.3333328  0.3333354 ]\n",
      " [0.33332893 0.33332938 0.33334166]\n",
      " [0.33332565 0.33332703 0.33334732]\n",
      " [0.33332813 0.33332846 0.3333434 ]\n",
      " [0.33333144 0.33333272 0.33333585]\n",
      " [0.3333313  0.3333326  0.33333614]\n",
      " [0.33332416 0.33332577 0.33335006]\n",
      " [0.3333235  0.33332512 0.3333514 ]\n",
      " [0.33332726 0.33332902 0.3333437 ]\n",
      " [0.33332515 0.333327   0.33334792]\n",
      " [0.3333304  0.333332   0.33333758]\n",
      " [0.33332896 0.3333302  0.33334085]\n",
      " [0.33332488 0.3333269  0.33334816]\n",
      " [0.33332175 0.33332375 0.33335447]\n",
      " [0.33331993 0.33332118 0.3333589 ]\n",
      " [0.33331978 0.33332092 0.33335936]\n",
      " [0.33332056 0.3333227  0.33335674]\n",
      " [0.33331943 0.33332148 0.33335912]\n",
      " [0.33332086 0.3333232  0.33335593]\n",
      " [0.33332133 0.33332375 0.3333549 ]\n",
      " [0.33332148 0.33332396 0.33335456]\n",
      " [0.33332664 0.33332652 0.33334678]\n",
      " [0.33331683 0.3333185  0.3333647 ]\n",
      " [0.33331677 0.3333182  0.33336502]\n",
      " [0.3333161  0.33331817 0.33336574]\n",
      " [0.33332363 0.33332345 0.3333529 ]\n",
      " [0.3333153  0.33331743 0.33336726]\n",
      " [0.3333196  0.33331972 0.33336067]\n",
      " [0.33331674 0.33331758 0.3333657 ]\n",
      " [0.33332583 0.33332568 0.3333485 ]\n",
      " [0.33332083 0.3333206  0.33335856]\n",
      " [0.33331746 0.33331788 0.3333647 ]\n",
      " [0.33332694 0.33332682 0.33334622]\n",
      " [0.33333272 0.33333272 0.33333457]\n",
      " [0.33333123 0.33333117 0.3333376 ]\n",
      " [0.3333332  0.33333322 0.33333355]\n",
      " [0.33332834 0.33332822 0.33334342]\n",
      " [0.33333284 0.33333284 0.33333433]\n",
      " [0.33333176 0.33333173 0.3333365 ]\n",
      " [0.3333321  0.33333215 0.33333576]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33332708 0.33332697 0.33334598]\n",
      " [0.33332348 0.3333241  0.33335242]\n",
      " [0.33332598 0.3333264  0.33334762]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33331823 0.33331916 0.33336264]\n",
      " [0.33332485 0.3333254  0.33334976]\n",
      " [0.3333178  0.33331874 0.33336344]\n",
      " [0.3333156  0.3333167  0.3333677 ]\n",
      " [0.33331585 0.33331692 0.33336726]\n",
      " [0.33331633 0.33331737 0.3333663 ]\n",
      " [0.33331937 0.33332023 0.33336037]\n",
      " [0.33331496 0.3333161  0.33336893]\n",
      " [0.33331487 0.333316   0.33336914]\n",
      " [0.3333146  0.33331576 0.33336967]\n",
      " [0.33331382 0.333315   0.33337113]\n",
      " [0.3333142  0.33331537 0.33337045]\n",
      " [0.33331612 0.33331716 0.3333667 ]\n",
      " [0.33331308 0.3333143  0.3333726 ]\n",
      " [0.33332264 0.33332574 0.33335155]\n",
      " [0.3333209  0.3333236  0.3333555 ]\n",
      " [0.3333125  0.33331382 0.3333737 ]\n",
      " [0.3333209  0.33332554 0.33335358]\n",
      " [0.33331248 0.33331752 0.33336997]\n",
      " [0.3333192  0.33332404 0.33335674]\n",
      " [0.33332294 0.3333265  0.3333506 ]\n",
      " [0.33331785 0.33332008 0.33336207]\n",
      " [0.33331195 0.3333172  0.33337083]\n",
      " [0.33332598 0.33333024 0.33334377]\n",
      " [0.33330488 0.3333102  0.3333849 ]\n",
      " [0.33331454 0.3333199  0.3333655 ]\n",
      " [0.3333028  0.33330813 0.333389  ]\n",
      " [0.33330676 0.33331233 0.33338088]\n",
      " [0.33330107 0.33330637 0.3333926 ]\n",
      " [0.33330554 0.3333112  0.33338326]\n",
      " [0.33329841 0.33330107 0.33340055]\n",
      " [0.33329985 0.33330527 0.33339489]\n",
      " [0.33329678 0.33330163 0.33340165]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333313 0.3333332  0.3333337 ]\n",
      " [0.33333284 0.33333296 0.33333424]\n",
      " [0.3333323  0.33333236 0.33333534]\n",
      " [0.33333206 0.3333323  0.3333356 ]\n",
      " [0.3333324  0.33333236 0.33333528]\n",
      " [0.33333105 0.33333132 0.3333376 ]\n",
      " [0.3333307  0.33333102 0.3333383 ]\n",
      " [0.3333325  0.33333248 0.333335  ]\n",
      " [0.33333206 0.33333206 0.3333359 ]\n",
      " [0.33333287 0.33333287 0.3333343 ]\n",
      " [0.3333315  0.33333147 0.333337  ]\n",
      " [0.33332917 0.3333294  0.33334145]\n",
      " [0.33333305 0.33333308 0.33333385]\n",
      " [0.33332998 0.3333299  0.3333401 ]\n",
      " [0.33333248 0.33333248 0.33333504]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333087 0.33333084 0.33333826]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3333324  0.33333245 0.33333516]\n",
      " [0.33333322 0.33333322 0.33333358]\n",
      " [0.3333299  0.3333298  0.3333403 ]\n",
      " [0.3333323  0.33333236 0.33333534]\n",
      " [0.33332914 0.3333294  0.33334148]\n",
      " [0.33333328 0.33333325 0.3333335 ]\n",
      " [0.33333167 0.33333176 0.33333656]\n",
      " [0.33333185 0.33333194 0.33333617]\n",
      " [0.33332655 0.33332697 0.33334652]\n",
      " [0.33332744 0.3333278  0.33334476]\n",
      " [0.33332735 0.3333277  0.3333449 ]\n",
      " [0.33332998 0.33333015 0.3333399 ]\n",
      " [0.33332595 0.3333264  0.33334762]\n",
      " [0.33332804 0.33332837 0.3333436 ]\n",
      " [0.33332565 0.33332613 0.3333482 ]\n",
      " [0.3333264  0.33332682 0.33334672]\n",
      " [0.33332452 0.33332506 0.3333504 ]\n",
      " [0.33332434 0.3333249  0.33335078]\n",
      " [0.33332404 0.3333246  0.33335137]\n",
      " [0.33332407 0.33332467 0.33335125]\n",
      " [0.33332407 0.33332467 0.33335125]\n",
      " [0.33332425 0.3333248  0.33335093]\n",
      " [0.3333248  0.33332533 0.33334988]\n",
      " [0.33332437 0.3333249  0.3333507 ]\n",
      " [0.33332592 0.33332705 0.33334702]\n",
      " [0.333325   0.33332753 0.3333475 ]\n",
      " [0.33332825 0.33333015 0.33334157]\n",
      " [0.33332276 0.33332342 0.33335382]\n",
      " [0.3333296  0.33333197 0.3333384 ]\n",
      " [0.33332232 0.33332297 0.3333547 ]\n",
      " [0.33332527 0.33332673 0.33334798]\n",
      " [0.3333277  0.3333298  0.33334252]\n",
      " [0.33331826 0.33332127 0.3333604 ]\n",
      " [0.33332616 0.33332807 0.33334577]\n",
      " [0.33332658 0.33332866 0.33334473]\n",
      " [0.33332735 0.3333302  0.3333424 ]\n",
      " [0.33332944 0.33333212 0.33333844]\n",
      " [0.33331442 0.33331755 0.33336797]\n",
      " [0.3333292  0.33333197 0.3333388 ]\n",
      " [0.33332887 0.33333176 0.33333933]\n",
      " [0.33332017 0.33332366 0.33335614]\n",
      " [0.3333187  0.3333223  0.33335903]\n",
      " [0.33331263 0.3333138  0.33337358]\n",
      " [0.33331004 0.33331245 0.33337754]\n",
      " [0.33331048 0.33331373 0.3333758 ]\n",
      " [0.333317   0.33332083 0.3333622 ]\n",
      " [0.33331    0.33331174 0.33337823]\n",
      " [0.33331168 0.33331543 0.3333729 ]\n",
      " [0.33331376 0.33331773 0.3333685 ]\n",
      " [0.33331522 0.3333152  0.33336964]\n",
      " [0.33330992 0.33331123 0.3333788 ]\n",
      " [0.333316   0.33331567 0.33336836]\n",
      " [0.33331212 0.3333126  0.3333753 ]\n",
      " [0.33331847 0.33331817 0.33336335]\n",
      " [0.3333077  0.3333094  0.33338287]\n",
      " [0.33330655 0.33330864 0.3333848 ]\n",
      " [0.3333224  0.3333222  0.33335543]\n",
      " [0.33330965 0.33331046 0.33337986]\n",
      " [0.3333122  0.3333123  0.33337548]\n",
      " [0.33331624 0.3333159  0.33336788]\n",
      " [0.33332354 0.33332336 0.3333531 ]\n",
      " [0.33332136 0.33332112 0.33335748]\n",
      " [0.33331242 0.33331224 0.33337533]\n",
      " [0.33332247 0.3333223  0.33335525]\n",
      " [0.33330745 0.33330834 0.33338425]\n",
      " [0.33332646 0.33332688 0.33334672]\n",
      " [0.3333177  0.3333174  0.33336487]\n",
      " [0.33332667 0.33332655 0.33334678]\n",
      " [0.33332586 0.3333263  0.33334786]\n",
      " [0.33333218 0.33333224 0.33333555]\n",
      " [0.33331165 0.33331296 0.3333754 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33331588 0.33331695 0.33336714]\n",
      " [0.3333165  0.33331755 0.33336592]\n",
      " [0.33332542 0.3333259  0.33334866]\n",
      " [0.33331046 0.3333119  0.3333777 ]\n",
      " [0.33331254 0.33331382 0.33337364]\n",
      " [0.33330905 0.33331054 0.33338043]\n",
      " [0.3333091  0.3333106  0.33338028]\n",
      " [0.33331695 0.33331972 0.33336338]\n",
      " [0.33331308 0.33331433 0.33337262]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333308 0.33333308 0.33333382]\n",
      " [0.33333293 0.33333296 0.3333341 ]\n",
      " [0.33333272 0.33333275 0.33333457]\n",
      " [0.33333254 0.3333326  0.33333486]\n",
      " [0.33333215 0.33333224 0.33333567]\n",
      " [0.33333203 0.33333212 0.33333582]\n",
      " [0.33333164 0.33333176 0.33333662]\n",
      " [0.33333197 0.33333218 0.33333588]\n",
      " [0.3333317  0.33333188 0.33333647]\n",
      " [0.33333254 0.33333302 0.33333445]\n",
      " [0.33333206 0.33333248 0.33333552]\n",
      " [0.33333236 0.33333296 0.33333465]\n",
      " [0.33333135 0.3333318  0.33333686]\n",
      " [0.33333093 0.3333313  0.3333378 ]\n",
      " [0.33333167 0.33333233 0.33333603]\n",
      " [0.33333203 0.33333284 0.33333513]\n",
      " [0.33333018 0.33333117 0.33333868]\n",
      " [0.33332655 0.33332732 0.33334613]\n",
      " [0.33332986 0.33333093 0.33333918]\n",
      " [0.33333185 0.33333284 0.33333528]\n",
      " [0.33332917 0.33333036 0.3333405 ]\n",
      " [0.33332866 0.33332995 0.3333414 ]\n",
      " [0.33332536 0.3333266  0.33334804]\n",
      " [0.333331   0.33333224 0.33333677]\n",
      " [0.33332467 0.33332604 0.33334926]\n",
      " [0.33332387 0.3333252  0.33335096]\n",
      " [0.33332324 0.3333245  0.33335227]\n",
      " [0.33332378 0.33332533 0.33335093]\n",
      " [0.33332255 0.33332393 0.33335355]\n",
      " [0.3333243  0.33332607 0.33334965]\n",
      " [0.3333217  0.3333231  0.33335522]\n",
      " [0.33332127 0.3333226  0.33335608]\n",
      " [0.33332294 0.3333233  0.33335373]\n",
      " [0.3333206  0.33332217 0.33335724]\n",
      " [0.33332556 0.33332542 0.33334896]\n",
      " [0.33333012 0.33333007 0.33333978]\n",
      " [0.33331987 0.33332175 0.33335838]\n",
      " [0.3333262  0.33332604 0.3333478 ]\n",
      " [0.33332855 0.3333285  0.3333429 ]\n",
      " [0.33332244 0.33332247 0.33335513]\n",
      " [0.3333244  0.33332425 0.33335128]\n",
      " [0.33332694 0.33332682 0.3333463 ]\n",
      " [0.333327   0.33332688 0.33334613]\n",
      " [0.3333281  0.33332804 0.33334383]\n",
      " [0.3333209  0.33332095 0.33335814]\n",
      " [0.33332804 0.33332837 0.33334363]\n",
      " [0.33332726 0.33332762 0.33334514]\n",
      " [0.33332998 0.33332992 0.33334008]\n",
      " [0.33332643 0.3333263  0.33334726]\n",
      " [0.3333266  0.33332703 0.3333464 ]\n",
      " [0.33333188 0.33333197 0.33333617]\n",
      " [0.3333262  0.3333266  0.33334717]\n",
      " [0.3333276  0.33332795 0.33334452]\n",
      " [0.33332145 0.3333222  0.33335632]\n",
      " [0.33332062 0.3333214  0.33335802]\n",
      " [0.33332598 0.33332643 0.33334753]\n",
      " [0.33332887 0.33332914 0.33334202]\n",
      " [0.33332875 0.33332902 0.3333422 ]\n",
      " [0.3333184  0.33331934 0.33336225]\n",
      " [0.33332336 0.333324   0.33335268]\n",
      " [0.3333192  0.33332008 0.3333608 ]\n",
      " [0.33331862 0.33331951 0.33336186]\n",
      " [0.3333174  0.33331835 0.33336428]\n",
      " [0.33332235 0.333323   0.33335465]\n",
      " [0.3333178  0.33331874 0.33336344]\n",
      " [0.3333196  0.33332047 0.33335996]\n",
      " [0.33331838 0.3333193  0.33336237]\n",
      " [0.3333174  0.33331838 0.3333642 ]\n",
      " [0.33331963 0.33332077 0.33335957]\n",
      " [0.3333217  0.3333237  0.33335462]\n",
      " [0.33331788 0.33331883 0.33336332]\n",
      " [0.33331856 0.33332276 0.3333587 ]\n",
      " [0.33332488 0.33332792 0.33334717]\n",
      " [0.33331978 0.33332145 0.33335876]\n",
      " [0.33332214 0.33332467 0.33335316]\n",
      " [0.33332625 0.33333015 0.33334357]\n",
      " [0.33332676 0.33333033 0.3333429 ]\n",
      " [0.33331642 0.33331746 0.33336616]\n",
      " [0.33330438 0.33330837 0.33338726]\n",
      " [0.33332294 0.3333273  0.33334973]\n",
      " [0.3333074  0.33331206 0.33338058]\n",
      " [0.33332533 0.33332956 0.33334512]\n",
      " [0.33330223 0.33330598 0.33339182]\n",
      " [0.33330753 0.33331245 0.33338   ]\n",
      " [0.33330333 0.3333079  0.33338878]\n",
      " [0.33331808 0.333323   0.3333589 ]\n",
      " [0.33330095 0.33330396 0.3333951 ]\n",
      " [0.3333002  0.3333039  0.3333959 ]\n",
      " [0.3333024  0.33330438 0.33339322]\n",
      " [0.3333031  0.33330816 0.33338878]\n",
      " [0.33330053 0.3333053  0.3333942 ]\n",
      " [0.3333189  0.33331862 0.33336252]\n",
      " [0.33330083 0.3333059  0.3333933 ]\n",
      " [0.33329853 0.33330166 0.33339983]\n",
      " [0.33331126 0.33331087 0.3333779 ]\n",
      " [0.33330163 0.33330318 0.3333952 ]\n",
      " [0.33330908 0.33330867 0.33338225]\n",
      " [0.33330017 0.33330202 0.3333978 ]\n",
      " [0.33331084 0.33331043 0.33337876]]\n",
      "loss 1.0986104\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "#dataset initialization\n",
    "nnfs.init()\n",
    "\n",
    "#creation of the Dense Layer class\n",
    "class Dense_Layer:\n",
    "    #weights and biases initialization\n",
    "    def __init__(self, num_features, num_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(num_features, num_neurons)\n",
    "        self.biases = np.zeros((1, num_neurons))\n",
    "\n",
    "    #perform the dot/matrix product calculations between the samples and the weights, add biases\n",
    "    def forward(self, samples):\n",
    "        self.outputs = np.dot(samples, self.weights) + self.biases\n",
    "\n",
    "#creation of the activation function\n",
    "class ReLU:\n",
    "    #rectified linear unit activation function\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "\n",
    "#creation of the softmax activation function\n",
    "class SoftMax:\n",
    "    '''compute the probability distributions of the output layer:\n",
    "            compute the exponentiated values for the output layer\n",
    "            normalize the exponentiated values for each sample in the output layer\n",
    "            '''\n",
    "    def forward(self, inputs):\n",
    "        '''subtract the input values from the largest value of the array\n",
    "            this is to stop from the values being too big. normalization will\n",
    "            be the same'''\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.outputs = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "#creation of the Loss class\n",
    "class Loss:\n",
    "    '''calculates the data and regularization losses, given model output and \n",
    "        ground truth values'''\n",
    "    def calculate(self, output, y):\n",
    "        #calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        #calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "#the specific loss we are using: Categorical Cross Entropy Loss\n",
    "class CategoricalCrossEntropy(Loss):\n",
    "    #calculate the cross entropy loss of the predicted values with the true values\n",
    "    def forward(self, y_pred, y_true):\n",
    "        #number of samples in the batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        '''clip data to prevent division by 0\n",
    "            clip both sides to not drage the mean towards any value'''\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7) \n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1) \n",
    "\n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "\n",
    "# getting the dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "print('this is first 5 samples in the dataset: \\n', X[:5], 'shape: ', X.shape)\n",
    "print('this is the first 5 values in the y: \\n', y[:5], 'shape: ', y.shape)\n",
    "\n",
    "\n",
    "#create layer 1, the reLU activation function. layer 2, and the softmax activation function\n",
    "dense_layer_1 = Dense_Layer(2, 3)\n",
    "reLU_activation = ReLU()\n",
    "dense_layer_2 = Dense_Layer(3, 3)\n",
    "softmax = SoftMax()\n",
    "loss_function = CategoricalCrossEntropy()\n",
    "\n",
    "dense_layer_1.forward(X)\n",
    "print('output values of the first layer: \\n', dense_layer_1.outputs[:5], 'shape: ', dense_layer_1.outputs.shape)\n",
    "\n",
    "reLU_activation.forward(dense_layer_1.outputs)\n",
    "print('output values after sent to the reLU activation function: \\n', reLU_activation.outputs[:5])\n",
    "\n",
    "dense_layer_2.forward(reLU_activation.outputs)\n",
    "print('output values of the second layer: \\n', dense_layer_2.outputs[:5])\n",
    "\n",
    "softmax.forward(dense_layer_2.outputs)\n",
    "print('output values after sent to the softmax activation function: \\n', softmax.outputs)\n",
    "\n",
    "loss = loss_function.calculate(softmax.outputs, y)\n",
    "print('loss', loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get ~0.33 values since the model is random, and its average loss is also not great for these data, as we've not yet trained out model on how to correct its errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***accuracy*** describes how often the largest confidence is the correct class in terms of a fraction.\n",
    "\n",
    "we will use the $ argmax $ values from the $ softmax outputs $ and then compare these to the targets. \n",
    "\n",
    "remember that the $ argmax $ returns the maximum value in the given function\n",
    "\n",
    "> it returns the index of where that largest confidence is in the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predictions:  [0 0 1]\n",
      "shape of target values, predictions:  (3,) (3,)\n",
      "acc:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "#target (ground truth) labels for the 3 samples\n",
    "class_targets = np.array([0,1,1])\n",
    "\n",
    "#calculate the values along the second axis (axis of 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1)\n",
    "\n",
    "print('the predictions: ', predictions)\n",
    "print('shape of target values, predictions: ', class_targets.shape, predictions.shape)\n",
    "\n",
    "'''if targets are one-hot encoded, convert them\n",
    "        we are handling one-hot encoded targets by converting them to sparse \n",
    "        values using np.argmax('''\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis=1)\n",
    "\n",
    "#true evaluates to 1, false to 0\n",
    "accuracy = np.mean(predictions==class_targets)\n",
    "\n",
    "print('acc: ', accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets add this code to the end of our neural network above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.34\n"
     ]
    }
   ],
   "source": [
    "'''calculate acuracy from output of softmax activation anf targets.\n",
    "    calculate values along the frist axis\n",
    "'''\n",
    "predictions = np.argmax(softmax.outputs, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "\n",
    "print('acc: ', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
