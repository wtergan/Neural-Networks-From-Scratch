{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Network Error with Loss\n",
    "\n",
    "In chapter two, our model is currently random. We need a way to calculate how wrong our neural network is at current prediction and begin adjusting the weights and biases to decrease error over time.\n",
    "\n",
    "To quantify how wrong our model is , we define it as the ***loss function***\n",
    "\n",
    "***loss function***\n",
    "\n",
    "> also referred to as the cost function, it quantifies how wrong the model is. We ideally want this loss to be 0.\n",
    "\n",
    "note that argmax applied to the output gives us the index of the biggest value in the softmax output. This index indicates the value with the biggest confidence. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Categorical Cross Entropy Loss*\n",
    "\n",
    ">  used to compare a 'ground-truth' probability (y) and some predicted distribution (y-hat or predictions). \n",
    "\n",
    "> one of the most commonly used loss functions with a softmax activation on the output layer.\n",
    "\n",
    "$ L = - \\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{M} y_{i,j} \\log(p_{i,j}) $,\n",
    "\n",
    "where:\n",
    "\n",
    "* $ N $ is the number of samples\n",
    "* $ M $ is the number of classes\n",
    "* $ y $ is the true label, a one-hot encoded vector of size $ M $\n",
    "* $ p $ is the predict label, a probabailiy distribution over the $ M $ classes\n",
    "* $ j $ is to index the classes.\n",
    "\n",
    "also denoted as:\n",
    "\n",
    "$ L = -  \\sum_{j} y_{i,j} \\log(p_{i,j}) $\n",
    "\n",
    "where:\n",
    "\n",
    "* $ L $ denotes the sample loss value\n",
    "* $ i $ is the i-th sample in the set\n",
    "* $ j $ is the label/output index\n",
    "* $ y $ denotes the target values\n",
    "* $ p $ denotes the predicted values\n",
    "\n",
    "which then is simplified further to:\n",
    "\n",
    "$ L = - \\log(p_{i,k}) $ , where $ k $ is an index of the 'true' probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
