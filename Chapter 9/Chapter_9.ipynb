{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "by calculating the gradients, which is the vector of all possible partial derivatives of a functiion with respect to all of its respective parameters, we can do the chain rule, which is the derivative of the chained functions, which are the product of the partial derivatives with respect to the loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets demonstrate the backpropagation by computing it at the ReLU activation output layer first. We will then do this for the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#the input values, its weights, and the bias\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above shows 2 vectors: the inputs and weights, and the bias.\n",
    "\n",
    "as you can see, the number of neurons for the subsequent layer will be 1, given that there is only one sample for x, and one set of weights for the inputs.\n",
    "\n",
    "(n_samples, num_features) * (num_features, n_neurons) = (n_samples, n_neurons)\n",
    "\n",
    "(1, 3) * (3, 1) = (1, 1), so single value for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, 2.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "input_weights = [x*w for x, w in zip(x, w)]\n",
    "print(input_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_weights represent the input and weight muliplication operation, multiplying each element in x with each element in w element-wise.\n",
    "\n",
    "z represents the sum of input_weights, plus the bias\n",
    "\n",
    "y represents the reLU activation function, which returns z if z > o, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output before reLU activation function: 6.0\n",
      "output after the reLU activation function:  6.0\n"
     ]
    }
   ],
   "source": [
    "z = sum(input_weights) + b\n",
    "print('output before reLU activation function:', z)\n",
    "\n",
    "y = max(z, 0)\n",
    "print('output after the reLU activation function: ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, lets compute the gradients:\n",
    "\n",
    "first, we must compute the derivative of the reLU function.\n",
    "\n",
    "> the derivative of reLU() with respect to its input z is:\n",
    "\n",
    "* 1 if z > 0, otherwise 0.\n",
    "\n",
    "we must take the derivative (gradient) from the next layer, in this case we will make up this value for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivative of the next layer is:  1.0\n",
      "the derivative of the current layer:  1\n",
      "the chain rule of the next layer with the current:  1.0\n"
     ]
    }
   ],
   "source": [
    "#derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "#derivative of ReLU and the chain rule, which is gradient of the next\n",
    "#layer multiplied by the derivative of the current layer\n",
    "drelu_dz = dvalue * (1 if z > 0 else 0)\n",
    "\n",
    "print('the derivative of the next layer is: ', dvalue)\n",
    "print('the derivative of the current layer: ', (1 if z > 0 else 0))\n",
    "print('the chain rule of the next layer with the current: ', drelu_dz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we have to calculate the partial derivative of the sum function, then use the chain rule to multiply this with the partial derivative of the reLU partial derivative.\n",
    "\n",
    "results will be:\n",
    "\n",
    "> drelu_dwx0: the partial derivative of the ReLU wrt. the first weighted input, w0x0\n",
    "\n",
    "> drelu_dwx1: the partial derivative of the ReLU wrt. the second weighted input, w1x1\n",
    "\n",
    "> drelu_dwx2: the partial derivative of the ReLU wrt. the third weighted input, w2x2\n",
    "\n",
    "> drelu_db: the partial derivative of the ReLU wrt. the bias, b\n",
    "\n",
    "the partial derivative of the sum operation is always 1, no matter the inputs.\n",
    "\n",
    "$ f(x, y) = x + y $\n",
    "\n",
    "partial derivative of the function with respect to x:\n",
    "$ \\frac{\\partial}{\\partial_x}f(x,y) = 1 $\n",
    "\n",
    "partial derivative of the function with respect to y:\n",
    "$ \\frac{\\partial}{\\partial_y}f(x,y) = 1 $\n",
    "\n",
    "thus, the partial derivative of the sum with respect to each of its input*weights are:\n",
    "\n",
    "> dsum_dxw0: 1\n",
    "\n",
    "> dsum_dxw1: 1\n",
    "\n",
    "> dsum_dxw2: 1\n",
    "\n",
    "> dsum_db: 1\n",
    "\n",
    "from here, we do the chain rule to find the derivative of the reLU with respect to each of the input*weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative of the sum function wrt. weighted inputs0:  1\n",
      "partial derivative of the sum function wrt. the weighted inputs1 1\n",
      "partial derivative of the sum function wrt the weighted inputs2:  1\n",
      "partial derivative of the sum function wrt the bias:  1\n",
      "partial derivative of the reLU function wrt the weighted inputs0 1.0\n",
      "partial derivative of the reLU function wrt the weighted inputs1 1.0\n",
      "partial derivative of the reLU function wrt the weighted inputs2 1.0\n",
      "partial derivative of the reLU function wrt the bias 1.0\n"
     ]
    }
   ],
   "source": [
    "#partial derivatives of the sum function wrt each of the weighted inputs,\n",
    "#and the bias. the sum operation is always 1\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "#using chain rule so that we know the partial derivatives of the reLU\n",
    "#function with respect to each of the weighted inputs. \n",
    "drelu_dwx0 = dsum_dxw0 * drelu_dz\n",
    "drelu_dwx1 = dsum_dxw1 * drelu_dz\n",
    "drelu_dwx2 = dsum_dxw2 * drelu_dz\n",
    "drelu_db = dsum_db * drelu_dz\n",
    "\n",
    "print('partial derivative of the sum function wrt. weighted inputs0: ', dsum_dxw0)\n",
    "print('partial derivative of the sum function wrt. the weighted inputs1', dsum_dxw1)\n",
    "print('partial derivative of the sum function wrt the weighted inputs2: ', dsum_dxw2)\n",
    "print('partial derivative of the sum function wrt the bias: ', dsum_db)\n",
    "\n",
    "print('partial derivative of the reLU function wrt the weighted inputs0', drelu_dwx0)\n",
    "print('partial derivative of the reLU function wrt the weighted inputs1', drelu_dwx1)\n",
    "print('partial derivative of the reLU function wrt the weighted inputs2', drelu_dwx2)\n",
    "print('partial derivative of the reLU function wrt the bias', drelu_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we must find the find the partial derivative of the multiplication function, then use the chain rule again.\n",
    "\n",
    "the partial derivative of the multiplicaiton function is whatever the input is being multiplied by. for example:\n",
    "\n",
    "$ f(x, y) = x * y $\n",
    "\n",
    "partial derivative of the function with respect to x:\n",
    "$ \\frac{\\partial}{\\partial_x}f(x,y) = y $\n",
    "\n",
    "partial derivative of the function with respect to y:\n",
    "$ \\frac{\\partial}{\\partial_y}f(x,y) = x $\n",
    "\n",
    "thus, the partial derivative of the first weighted inputs with respect to the input is the weight\n",
    "\n",
    "then, we apply chain rule and multiply this partial with the partial of the subsequent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative of the multiplication function wrt x0:  -3.0\n",
      "partial derivative of the multiplication function wrt x1:  -1.0\n",
      "partial derivative of the multiplication function wrt x2:  2.0 \n",
      "\n",
      "partial derivative of the multiplication function wrt w0:  1.0\n",
      "partial derivative of the multiplication function wrt w1:  -2.0\n",
      "partial derivative of the multiplication function wrt x2:  2.0 \n",
      "\n",
      "partial derivative of the reLU function with resepct to the input 0:  -3.0\n",
      "partial derivative of the reLU function with resepct to the weight 0:  1.0\n",
      "partial derivative of the reLU function with resepct to the input 1:  -1.0\n",
      "partial derivative of the reLU function with resepct to the weight 1:  -2.0\n",
      "partial derivative of the reLU function with resepct to the input 2:  2.0\n",
      "partial derivative of the reLU function with resepct to the weight 2:  3.0\n",
      "partial derivatve of the reLU function with respect to the bias:  1.0\n"
     ]
    }
   ],
   "source": [
    "#partial derivatives of the multiplication function wrt. each of the inputs:\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "\n",
    "#you do not need to do the partial derivative of the bias, because it isnt being multiplied.\n",
    "print('partial derivative of the multiplication function wrt x0: ', dmul_dx0)\n",
    "print('partial derivative of the multiplication function wrt x1: ', dmul_dx1)\n",
    "print('partial derivative of the multiplication function wrt x2: ', dmul_dx2, '\\n')\n",
    "\n",
    "\n",
    "#now, do chain rule of these derivatives with the subsequent one we did previously\n",
    "drelu_dx0 = drelu_dwx0 * dmul_dx0\n",
    "drelu_dx1 = drelu_dwx1 * dmul_dx1\n",
    "drelu_dx2 = drelu_dwx2 * dmul_dx2\n",
    "\n",
    "#partial derivatives of the multiplication function wrt. each of the weights:\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "\n",
    "print('partial derivative of the multiplication function wrt w0: ', dmul_dw0)\n",
    "print('partial derivative of the multiplication function wrt w1: ', dmul_dw1)\n",
    "print('partial derivative of the multiplication function wrt x2: ', dmul_dx2, '\\n')\n",
    "\n",
    "#now, do chain rule of these derivatives with subsequent one we did preiously\n",
    "drelu_dw0 = drelu_dwx0 * dmul_dw0\n",
    "drelu_dw1 = drelu_dwx0 * dmul_dw1\n",
    "drelu_dw2 = drelu_dwx0 * dmul_dw2\n",
    "\n",
    "print('partial derivative of the reLU function with resepct to the input 0: ', drelu_dx0)\n",
    "print('partial derivative of the reLU function with resepct to the weight 0: ', drelu_dw0)\n",
    "print('partial derivative of the reLU function with resepct to the input 1: ', drelu_dx1)\n",
    "print('partial derivative of the reLU function with resepct to the weight 1: ', drelu_dw1)\n",
    "print('partial derivative of the reLU function with resepct to the input 2: ', drelu_dx2)\n",
    "print('partial derivative of the reLU function with resepct to the weight 2: ', drelu_dw2)\n",
    "\n",
    "print('partial derivatve of the reLU function with respect to the bias: ', drelu_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we will uses these partial derivatives, (which is the gradient), to the weights to minimize the output. this is referred to as the optimizer\n",
    "\n",
    "in this case, we will apply a negative fraction to the gradients of the weights, since we want to decrease the final output value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets show the current values of the weights and the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let now modify the weight values using the gradients of each respective weights:\n",
    "\n",
    "> changed the weights and biases slightly as to decrease the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0029999999999997, -0.994, 1.9910000000000003] 0.998\n"
     ]
    }
   ],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db\n",
    "\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do another forward pass and see the changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0029999999999997, 1.988, 5.973000000000001]\n"
     ]
    }
   ],
   "source": [
    "#multiplying the inputs and weights\n",
    "input_weights = [x*w for x, w in zip(x, w)]\n",
    "print(input_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output before the reLU activation function:  5.956000000000001\n",
      "output after the reLU activation function:  5.956000000000001\n"
     ]
    }
   ],
   "source": [
    "#adding (performing the dot product, adding the bias)\n",
    "z = sum(input_weights) + b\n",
    "print('output before the reLU activation function: ', z)\n",
    "\n",
    "#reLU activation function\n",
    "y = max(z, 0)\n",
    "print('output after the reLU activation function: ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above code, we have minimized the reLU output. In a real world application, we do not minimize this layer, but rather the loss function. Remember we only did this for the reLU for demonstation purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop with multiple neurons\n",
    "\n",
    "now, instead of a single neuron, we have a layer with multiple neurons. During backprop, wach neuron from the current layer will recieve a vector of partial deriviatives instead of a sinlge value. \n",
    "\n",
    "below is the code to show this: \n",
    "\n",
    "> take the transposed weights, which are the transposed aray of the derivatives wrt the inputs, and multiply them by their respective gradients (related to the given neurons) to apply the chain rule.\n",
    "\n",
    "> then, we sum along with the inputs.\n",
    "\n",
    "> calculate the gradient for the next layer in the backpropogation. the next layer is the previous layer in the order of creation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients from the next layer:  [1. 1. 1.]\n",
      "dvalue shape:  (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#passed in gradient from the next layer. use vector of 1s for this example\n",
    "#remember, this will have to be a 2D array of rows and columns.\n",
    "dvalues = np.array([1., 1., 1.])\n",
    "print('gradients from the next layer: ', dvalues)\n",
    "print('dvalue shape: ', dvalues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the weights: \n",
      " [[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  10.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]]\n",
      "this is the weights shape (3, 4)\n",
      "this is the weights transposed: \n",
      " [[ 0.2   0.5  -0.26]\n",
      " [ 0.8  10.91 -0.27]\n",
      " [-0.5   0.26  0.17]\n",
      " [ 1.   -0.5   0.87]]\n",
      "this is now the weights shape:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "#we have 3 sets of weights, one set for each neuron. 4 inputs, thus 4 weights.\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], [0.5, 10.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]])\n",
    "print('this is the weights: \\n', weights)\n",
    "print('this is the weight''s shape', weights.shape)\n",
    "weights_T = weights.T\n",
    "print('this is the weights transposed: \\n', weights_T)\n",
    "print('this is now the weight''s shape: ', weights_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 11.44 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "#sum weights of the given input. multiply the passed in gradient for this neuron\n",
    "dx0 = sum(weights_T[0]) * dvalues[0]\n",
    "dx1 = sum(weights_T[1]) * dvalues[0]\n",
    "dx2 = sum(weights_T[2]) * dvalues[0]\n",
    "dx3 = sum(weights_T[3]) * dvalues[0]\n",
    "\n",
    "#gradient of the neuron's function wrt the inputs\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dinputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sum of the multiplicaiton of the elements is the dot product. we can achieve same by doing np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[ 0.44 11.44 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([1,1,1])\n",
    "print(dvalues.shape)\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], [0.5, 10.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "#sum weights of the given inputs, multiply the passed in each gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights)\n",
    "print(dinputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, lets account for batches of samples. Above, we are using a single sample responsible for a single gradient that is backpropagated between alyers. The row vector that we created for dvalues is in proparation for the batch of data.\n",
    "\n",
    "with more samples, the layer will return a list of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[ 0.44 11.44 -0.07  1.37]\n",
      " [ 0.88 22.88 -0.14  2.74]\n",
      " [ 1.32 34.32 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "print(dvalues.shape)\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], [0.5, 10.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "dinputs = np.dot(dvalues, weights)\n",
    "print(dinputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets combine the forward and backward pass of a single neuron with a full layer and batched-based partial derivatives. We'll minimize ReLU's output, once again, for this example only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 4)\n",
      "weights shape:  (4, 3)\n",
      "layer outputs before reLU activation function: \n",
      " [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "layer outputs after the reLU activation function: \n",
      " [[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]]\n",
      "[[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]]\n",
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#passed in gradient from the next layer, array of incremental gradient values\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "\n",
    "#we have 3 sets of inputs - samples\n",
    "inputs =  np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "print('input shape: ', inputs.shape)\n",
    "\n",
    "'''we have 3 sets of weights - one set for each neuron,\n",
    "   we have 4 inputs, thus 4 weights for each neuron'''\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "print('weights shape: ', weights.shape)\n",
    "\n",
    "#one bias for each neuron, biases are just a row vector of the shape (1, num_neurons)\n",
    "biases = np.array([[2,3,0.5]])\n",
    "\n",
    "#forward pass: perform the dot product, then perform the reLU activation function\n",
    "layer_outputs = np.dot(inputs, weights) + biases\n",
    "print('layer outputs before reLU activation function: \\n', layer_outputs)\n",
    "relu_outputs = np.maximum(0, layer_outputs)\n",
    "print('layer outputs after the reLU activation function: \\n', relu_outputs)\n",
    "\n",
    "'''lets optimize and test the backpropagation.\n",
    "    ReLU activation simulates derivative wrt the input values from the next layer\n",
    "    passed to the current layer during backpropagation\n",
    "'''\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "print(drelu)\n",
    "\n",
    "#Dense layer: \n",
    "#dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "#dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "#dbiases - sum values, do this over the samples (first axis), keepdims.\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "#update parameters\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we will update the dense layer and ReLU activation code with a backward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current weights: [[ 0.2   0.5  -0.26]\n",
      " [ 0.8  -0.91 -0.27]\n",
      " [-0.5   0.26  0.17]\n",
      " [ 1.   -0.5   0.87]]\n",
      "current biases: [[2.  3.  0.5]]\n",
      "current sample(s): [[ 1.   2.   3.   2.5]\n",
      " [ 2.   5.  -1.   2. ]\n",
      " [-1.5  2.7  3.3 -0.8]]\n",
      "output before reLU activation function: [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "output after reLU activation function: [[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]]\n",
      "backward  [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "derivative of the reLU: \n",
      "[[1 1 1]\n",
      " [2 0 2]\n",
      " [3 3 3]]\n",
      "current weights: [[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n",
      "current bias: [[6 6 6]]\n",
      "current inputs: [[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, num_features, num_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(num_features, num_neurons)\n",
    "        self.biases = np.zeros((1, num_neurons))\n",
    "\n",
    "    #remember, we need to remember what the inputs were, when doing backpropagation\n",
    "    def forward(self, samples):\n",
    "        self.outputs = np.dot(samples, self.weights) + self.biases\n",
    "        self.samples = samples\n",
    "\n",
    "    #it takes in the gradients from the next layer\n",
    "    def backward(self, dvalues):\n",
    "        #gradients on parameters\n",
    "        self.dweights = np.dot(self.samples.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        #gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class reLU:\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #since we need to modify the original variable, lets make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        #zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "X  = np.array([[1, 2, 3, 2.5],\n",
    "                [2., 5., -1., 2],\n",
    "                [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "\n",
    "dense1 = Dense_Layer(3, 3)\n",
    "\n",
    "dense1.weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "dense1.biases = np.array([[2,3,0.5]])\n",
    "\n",
    "print(f'current weights: {dense1.weights}\\ncurrent biases: {dense1.biases}')\n",
    "dense1.forward(X)\n",
    "print(f'current sample(s): {dense1.samples}')\n",
    "print(f'output before reLU activation function: {dense1.outputs}')\n",
    "activation1 = reLU()\n",
    "activation1.forward(dense1.outputs)\n",
    "print(f'output after reLU activation function: {activation1.outputs}')\n",
    "\n",
    "activation1.backward(dvalues)\n",
    "print('backward ', activation1.inputs)\n",
    "print(f'derivative of the reLU: \\n{activation1.dinputs}')\n",
    "\n",
    "dense1.backward(dvalues)\n",
    "print(f'current weights: {dense1.dweights}\\ncurrent bias: {dense1.dbiases}\\ncurrent inputs: {dense1.dinputs}')\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### implementation of the Categorical Cross-Entropy Loss derivative code\n",
    "\n",
    "we want to compute the derivative of the loss function with respect to its inputs, which would be the softmax outputs.\n",
    "\n",
    "the derivative of the loss function is -(y_true/y_pred).\n",
    "\n",
    "we will also implement the rest of the code so far for further practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do the derivative of the loss, we must first check how many dimensions the y_true consists o.\n",
    "\n",
    "if the shape of the labels returns 1, that must mean that it is a one hot vector, thus we need to change it to a list of one hot vectors instead. so we will yse the .eye method from the numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code explanation of the derivative of the loss.\n",
    "import numpy as np\n",
    "np.eye(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, let index this table with the numerical label to get the one hot coded vector that it represents. remember, if the y_true is already encoded, we do not need to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the one hot vector for the second row in the array: [0. 1. 0. 0. 0.]\n",
      "this is the one hot vector for the fourth row in the array: [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(f'this is the one hot vector for the second row in the array: {np.eye(5)[1]}')\n",
    "print(f'this is the one hot vector for the fourth row in the array: {np.eye(5)[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = [0,1,2,1,0]\n",
    "y = np.eye(len(y))[y]\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we perform the gradient normalization. divide all of the gradients by the number of samples, so that the gradients are normalized and this makes the sum's magnitude invariant to the number of samples. the sum will be the optimizer. if the gradients arent normalized, when we do the optimization for a large number of samples, we will have the adjust the learning rates according to each set of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#creation of the dense layer class\n",
    "class Dense_Layer:\n",
    "    #initialization of the weights and biases.\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(num_inputs, num_neurons)\n",
    "        self.biases = 0.01 * np.zeros((1, num_neurons))\n",
    "\n",
    "    #compute the matrix product of the inputs and the weights.\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "\n",
    "    #take gradients of next layer, compute gradients wrt parameters.\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        self.dweighhts = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "#creation of the reLU activation function class.\n",
    "class reLU:\n",
    "    #perform the reLU activation function on the outputs.\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    #takes gradients from next layer, compute gradients wrt parameters.\n",
    "    def backward(self, dvalues):\n",
    "        #if inputs values <=0, set to zero for gradients.\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "#creation of the softmax activation function class.\n",
    "class SoftMax:\n",
    "    #compute the softmax to get the probability distribution of the outputs.\n",
    "    def forward(self, inputs):\n",
    "        #normalize the exp values by subtracting max from each row with the row.\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    #derivative of softmax is wrt the inputs is softmax * (1-softmax)\n",
    "    def backward(self, dvalues):\n",
    "        pass\n",
    "\n",
    "#creation of the Categorical Cross Entropy Loss function class.\n",
    "class CCrossEntropyLoss:\n",
    "    #compute the loss of the outputs and the true label class.\n",
    "    def forward(self, outputs, y):\n",
    "        #clip the data\n",
    "        outputs_clipped = np.clip(outputs, 1e-7, 1-1e-7)\n",
    "        #compute the confidence scor es depending on the shape of the true class.\n",
    "        if len(y.shape)==1: conf =outputs_clipped[range(len(outputs)), y]\n",
    "        elif len(y.shape)==2: conf = np.sum(outputs*y, axis=1)\n",
    "        #compute the sample losses and the average losss.\n",
    "        sample_losses = -np.log(conf)\n",
    "        average_loss = np.mean(sample_losses)\n",
    "\n",
    "    #derivative of the loss function wrt its inputs is -(y_true/y_pred).\n",
    "    def backward(self, dvalues, y):\n",
    "        #number of samples.\n",
    "        samples = len(dvalues)\n",
    "        #number of labels per sample,\n",
    "        labels = len(dvalues[0])\n",
    "        #if labels are one dimensional, make them into a one hot vector.\n",
    "        if len(y.shape)==1: y = np.eye(labels)[y]\n",
    "        #compute and normalize the gradients\n",
    "        self.dinputs = -y / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of the SoftMax activation derivative\n",
    "\n",
    "now, we want to compute the derivative of the softmax activation wrt its inputs (which will be the outputs from the second dense layer in this case)\n",
    "\n",
    "the derivative of the softmax is : softmax * (1 - softmax)\n",
    "\n",
    "lets demonstate this with some example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7]\n",
      " [0.1]\n",
      " [0.2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_output =  [0.7, 0.1, 0.2]\n",
    "\n",
    "softmax_output = np.array(softmax_output).reshape(-1, 1)\n",
    "print(softmax_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the left side of the equation is softmax's output multiplied by the Kronecker delta\n",
    "\n",
    "> equals 1 when both inputs are equal\n",
    "\n",
    "> equals 0 otherwise.\n",
    "\n",
    "we can visualize this as an array, where its an aray of zeros with the one in the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(softmax_output.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we do the multiplication of both of the values from the equation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_output * np.eye(softmax_output.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can speed this up by using the np.diagflat method: creates an arrya using an input vector as the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the other part of the equation is the multiplication of the softmax outputs, iterating over the j and k indices respectively. since, for each sample (the i index), we have to multiply the values from the softmax function's output, we can use the dot product operation. for this, we must transpose the second argument to get its row vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we can perform the subtraction of both arrays following the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21 -0.07 -0.14]\n",
      " [-0.07  0.09 -0.02]\n",
      " [-0.14 -0.02  0.16]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the matrix result of the equation and the array solution provided by the code is call the jacobian matrix. in our case, this matrix is an array of partial derivatives in all of the combinations of both input vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax:\n",
    "    #compute the softmax to get the probability distribution of the outputs.\n",
    "    def forward(self, inputs):\n",
    "        #normalize the exp values by subtracting max from each row with the row.\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "    #compute the gradients of the softmax wrt the inputs (dvalues).\n",
    "    def backward(self, dvalues):\n",
    "        #create an unitialized array.\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        #enumerate outputs and gradients.\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            #flatten the output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            #calculate the jacobian matrix of the output.\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            #calculate the sample-wise gradient and add that to array of sample gradients.\n",
    "            self.dinputs[index] = np.dot(jacobian, single_dvalues)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the above code we:\n",
    "\n",
    "> created an empty array, which will become the resultant gradient array\n",
    "\n",
    "* this will be the same shape as the gradients that we are recieving to apply the chain rule.\n",
    "\n",
    "> then we will iterate sample-wise over pairs of the outputs and gradients, calculating the partial derivatives as described ealier and calculating the final product (chain rule) of the jacobian matrix and gradient vector.\n",
    "\n",
    "* we store the resulting vector as a row in the dinput array.\n",
    "\n",
    "> we store each vector in each row while iterating, forming the output array."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do another example of this using simpler values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape: (3, 4)\n",
      "probabilities:\n",
      " [[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.0320586  0.08714432 0.23688282 0.64391426]]\n",
      "probabilities shape: (3, 4)\n",
      "[[0.0320586 ]\n",
      " [0.08714432]\n",
      " [0.23688282]\n",
      " [0.64391426]]\n",
      "[[0.0320586 ]\n",
      " [0.08714432]\n",
      " [0.23688282]\n",
      " [0.64391426]]\n",
      "[[0.0320586 ]\n",
      " [0.08714432]\n",
      " [0.23688282]\n",
      " [0.64391426]]\n",
      "[[-0.0079911  -0.01300762 -0.0116701   0.03266881]\n",
      " [-0.0079911  -0.01300762 -0.0116701   0.03266881]\n",
      " [-0.0079911  -0.01300762 -0.0116701   0.03266881]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#get the inputs\n",
    "inputs = np.array([[1,2,3,4], [2,3,4,5], [3,4,5,6]])\n",
    "print(f'inputs shape: {inputs.shape}')\n",
    "\n",
    "#compute the probabilities of each inputs.\n",
    "exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "print(f'probabilities:\\n {probabilities}\\nprobabilities shape: {probabilities.shape}')\n",
    "\n",
    "#lets assume the gradients of the loss wrt. the probabilities are:\n",
    "dvalues = np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "            [0.2, 0.3, 0.4, 0.5],\n",
    "            [0.3, 0.4, 0.5, 0.6]])\n",
    "\n",
    "dinputs = np.empty_like(dvalues)\n",
    "\n",
    "'''for each samples's probabilities and its corresponding dvalues, it:\n",
    "        -reshape the sample's probabilities to be a row vector. flatten it.\n",
    "        -compute the jacobian matrix of the sample's probabilities\n",
    "        -use the jacobian matrix to compute the sample's gradient\n",
    "            which is the dot product of the jacobian matrix and the \n",
    "            corresoponding dvalue\n",
    "        -add this sample's gradient to the empty sample gradients.\n",
    "'''\n",
    "for index, (single_output, single_dvalues) in enumerate(zip(probabilities, dvalues)):\n",
    "    single_output = single_output.reshape(-1,1)\n",
    "    print(single_output)\n",
    "    jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "    sample_gradients = np.dot(jacobian, single_dvalues)\n",
    "    dinputs[index] = sample_gradients\n",
    "\n",
    "print(dinputs)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Categorical Cross Entropy Loss and Softmax activation derivative code\n",
    "\n",
    "we can simplify the derivative of both of these functions by combining them:\n",
    "\n",
    "> if we apply the chain rule to both partial derivatives, the whoel equation simplifies significantly to the subtraction of the predicted anf ground truth values. \n",
    "\n",
    "> faster to compute\n",
    "\n",
    "we can create a single class that computes the functions, then do the backward method, which calculates the combined gradient of the loss and activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining the softmax classifieer and the cross entropy loss for faster compute.\n",
    "class SoftMaxLossClass:\n",
    "    #creates the activation and loss function objects.\n",
    "    def __init__(self):\n",
    "        self.activation = SoftMax()\n",
    "        self.loss = CCrossEntropyLoss()\n",
    "\n",
    "    #forward pass.\n",
    "    def forward(self, inputs, y):\n",
    "        self.activation.forward(inputs)\n",
    "        self.outputs = self.activation.output\n",
    "        return self.loss.forward(self.outputs, y)\n",
    "    \n",
    "    #backward pass.\n",
    "    def backward(self, dvalues, y):\n",
    "        #number of samples.\n",
    "        samples = len(dvalues)\n",
    "        #if labels are one-hot encoded, turn them to discrete values.\n",
    "        if len(y.shape)==2: y = np.argmax(y, axis=1)\n",
    "        #copy the dvalues into gradient inputs\n",
    "        self.dinput = dvalues.copy()\n",
    "        #compute the gradient.\n",
    "        self.dinputs[range(samples), y] -= 1\n",
    "        #normalize the gradients.\n",
    "        self.dinputs = self.dinput / samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "#creation of the dense layer class.\n",
    "class Dense_Layer:\n",
    "    #initialization of the weights, biases\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = 0.01 * np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #perform the matrix product of the weights and inputs, add biases.\n",
    "        self.outputs = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        #take gradients from next layer, get gradients wrt. parameters.\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=1, keepdims=True)\n",
    "\n",
    "#creation of the reLU activation function.\n",
    "class reLU:\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #compute gradients of the reLU wrt. gradients from next layer.\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "#creation of the softmax activation function.\n",
    "class SoftMax:\n",
    "    def forward(self, inputs):\n",
    "        #compute the softmax to ger the probability distribution for each sample.\n",
    "        exp_values = np.exp(inputs, np.max(inputs, axis=1, keepdims=True))\n",
    "        self.probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        #computes the gradient of softmax wrt. its inputs.\n",
    "        #initialize dinputs to be empty array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.outputs, dvalues)):\n",
    "            #change the sample output to be a row vector.\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            #compute the jacobian matrix of the sample output.\n",
    "            jacobian = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            #calculate the sample-wise gradient, add that to self.dinputs.\n",
    "            sample_grad = np.dot(jacobian, single_dvalues)\n",
    "            self.dinputs[index] = sample_grad\n",
    "\n",
    "#creation of the Categorical Cross Entropy Loss function.\n",
    "class CCrossEntropyLoss:\n",
    "    def forward(self, outputs, y):\n",
    "        #clip the data so no outputs are exactly 0 or 1.\n",
    "        outputs_clipped = outputs_clipped[outputs, 1e-7, 1-1e-7]\n",
    "        #confidence scores, depends on shape of the true values.\n",
    "        if len(y.shape)==1: conf = outputs_clipped[range(len(outputs)), y]\n",
    "        elif len(y.shape)==2: conf =  np.sum(outputs_clipped* y, axis=1)\n",
    "        #compute the sample and average losses.\n",
    "        sample_losses = -np.log(conf)\n",
    "        average_loss = np.mean(sample_losses)\n",
    "\n",
    "    def backward(self, dvalues, y):\n",
    "        #derivative of loss function wrt. its inputs is -(y_true/y_pred)\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        #if y is not a one hot vector, change it into a one vector.\n",
    "        if len(y.shape)==1: y = np.eye(labels)[y]\n",
    "        #compute the gradients.\n",
    "        self.dinputs = -(y/dvalues)\n",
    "        #normalize the gradients.\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "#creation of the combination of the softmax and the ccentropy loss classes.\n",
    "class SoftMax_Loss_Class:\n",
    "    def forward(self, inputs, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
