{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "by calculating the gradients, which is the vector of all possible partial derivatives of a functiion with respect to all of its respective parameters, we can do the chain rule, which is the derivative of the chained functions, which are the product of the partial derivatives with respect to the loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets demonstrate the backpropagation by computing it at the ReLU activation output layer first. We will then do this for the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#the input values, its weights, and the bias\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above shows 2 vectors: the inputs and weights, and the bias.\n",
    "\n",
    "as you can see, the number of neurons for the subsequent layer will be 1, given that there is only one sample for x, and one set of weights for the inputs.\n",
    "\n",
    "(n_samples, num_features) * (num_features, n_neurons) = (n_samples, n_neurons)\n",
    "\n",
    "(1, 3) * (3, 1) = (1, 1), so single value for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, 2.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "input_weights = [x*w for x, w in zip(x, w)]\n",
    "print(input_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_weights represent the input and weight muliplication operation, multiplying each element in x with each element in w element-wise.\n",
    "\n",
    "z represents the sum of input_weights, plus the bias\n",
    "\n",
    "y represents the reLU activation function, which returns z if z > o, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output before reLU activation function: 6.0\n",
      "output after the reLU activation function:  6.0\n"
     ]
    }
   ],
   "source": [
    "z = sum(input_weights) + b\n",
    "print('output before reLU activation function:', z)\n",
    "\n",
    "y = max(z, 0)\n",
    "print('output after the reLU activation function: ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, lets compute the gradients:\n",
    "\n",
    "first, we must compute the derivative of the reLU function.\n",
    "\n",
    "> the derivative of reLU() with respect to its input z is:\n",
    "\n",
    "* 1 if z > 0, otherwise 0.\n",
    "\n",
    "we must take the derivative (gradient) from the next layer, in this case we will make up this value for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the derivative of the next layer is:  1.0\n",
      "the derivative of the current layer:  1\n",
      "the chain rule of the next layer with the current:  1.0\n"
     ]
    }
   ],
   "source": [
    "#derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "#derivative of ReLU and the chain rule, which is gradient of the next\n",
    "#layer multiplied by the derivative of the current layer\n",
    "drelu_dz = dvalue * (1 if z > 0 else 0)\n",
    "\n",
    "print('the derivative of the next layer is: ', dvalue)\n",
    "print('the derivative of the current layer: ', (1 if z > 0 else 0))\n",
    "print('the chain rule of the next layer with the current: ', drelu_dz)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we have to calculate the partial derivative of the sum function, then use the chain rule to multiply this with the partial derivative of the reLU partial derivative.\n",
    "\n",
    "results will be:\n",
    "\n",
    "> drelu_dwx0: the partial derivative of the ReLU wrt. the first weighted input, w0x0\n",
    "\n",
    "> drelu_dwx1: the partial derivative of the ReLU wrt. the second weighted input, w1x1\n",
    "\n",
    "> drelu_dwx2: the partial derivative of the ReLU wrt. the third weighted input, w2x2\n",
    "\n",
    "> drelu_db: the partial derivative of the ReLU wrt. the bias, b\n",
    "\n",
    "the partial derivative of the sum operation is always 1, no matter the inputs.\n",
    "\n",
    "$ f(x, y) = x + y $\n",
    "\n",
    "partial derivative of the function with respect to x:\n",
    "$ \\frac{\\partial}{\\partial_x}f(x,y) = 1 $\n",
    "\n",
    "partial derivative of the function with respect to y:\n",
    "$ \\frac{\\partial}{\\partial_y}f(x,y) = 1 $\n",
    "\n",
    "thus, the partial derivative of the sum with respect to each of its input*weights are:\n",
    "\n",
    "> dsum_dxw0: 1\n",
    "\n",
    "> dsum_dxw1: 1\n",
    "\n",
    "> dsum_dxw2: 1\n",
    "\n",
    "> dsum_db: 1\n",
    "\n",
    "from here, we do the chain rule to find the derivative of the reLU with respect to each of the input*weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative of the sum function wrt. weighted inputs0:  1\n",
      "partial derivative of the sum function wrt. the weighted inputs1 1\n",
      "partial derivative of the sum function wrt the weighted inputs2:  1\n",
      "partial derivative of the sum function wrt the bias:  1\n",
      "partial derivative of the reLU function wrt the weighted inputs0 1.0\n",
      "partial derivative of the reLU function wrt the weighted inputs1 1.0\n",
      "partial derivative of the reLU function wrt the weighted inputs2 1.0\n",
      "partial derivative of the reLU function wrt the bias 1.0\n"
     ]
    }
   ],
   "source": [
    "#partial derivatives of the sum function wrt each of the weighted inputs,\n",
    "#and the bias. the sum operation is always 1\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "#using chain rule so that we know the partial derivatives of the reLU\n",
    "#function with respect to each of the weighted inputs. \n",
    "drelu_dwx0 = dsum_dxw0 * drelu_dz\n",
    "drelu_dwx1 = dsum_dxw1 * drelu_dz\n",
    "drelu_dwx2 = dsum_dxw2 * drelu_dz\n",
    "drelu_db = dsum_db * drelu_dz\n",
    "\n",
    "print('partial derivative of the sum function wrt. weighted inputs0: ', dsum_dxw0)\n",
    "print('partial derivative of the sum function wrt. the weighted inputs1', dsum_dxw1)\n",
    "print('partial derivative of the sum function wrt the weighted inputs2: ', dsum_dxw2)\n",
    "print('partial derivative of the sum function wrt the bias: ', dsum_db)\n",
    "\n",
    "print('partial derivative of the reLU function wrt the weighted inputs0', drelu_dwx0)\n",
    "print('partial derivative of the reLU function wrt the weighted inputs1', drelu_dwx1)\n",
    "print('partial derivative of the reLU function wrt the weighted inputs2', drelu_dwx2)\n",
    "print('partial derivative of the reLU function wrt the bias', drelu_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we must find the find the partial derivative of the multiplication function, then use the chain rule again.\n",
    "\n",
    "the partial derivative of the multiplicaiton function is whatever the input is being multiplied by. for example:\n",
    "\n",
    "$ f(x, y) = x * y $\n",
    "\n",
    "partial derivative of the function with respect to x:\n",
    "$ \\frac{\\partial}{\\partial_x}f(x,y) = y $\n",
    "\n",
    "partial derivative of the function with respect to y:\n",
    "$ \\frac{\\partial}{\\partial_y}f(x,y) = x $\n",
    "\n",
    "thus, the partial derivative of the first weighted inputs with respect to the input is the weight\n",
    "\n",
    "then, we apply chain rule and multiply this partial with the partial of the subsequent function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partial derivative of the multiplication function wrt x0:  -3.0\n",
      "partial derivative of the multiplication function wrt x1:  -1.0\n",
      "partial derivative of the multiplication function wrt x2:  2.0 \n",
      "\n",
      "partial derivative of the multiplication function wrt w0:  1.0\n",
      "partial derivative of the multiplication function wrt w1:  -2.0\n",
      "partial derivative of the multiplication function wrt x2:  2.0 \n",
      "\n",
      "partial derivative of the reLU function with resepct to the input 0:  -3.0\n",
      "partial derivative of the reLU function with resepct to the weight 0:  1.0\n",
      "partial derivative of the reLU function with resepct to the input 1:  -1.0\n",
      "partial derivative of the reLU function with resepct to the weight 1:  -2.0\n",
      "partial derivative of the reLU function with resepct to the input 2:  2.0\n",
      "partial derivative of the reLU function with resepct to the weight 2:  3.0\n",
      "partial derivatve of the reLU function with respect to the bias:  1.0\n"
     ]
    }
   ],
   "source": [
    "#partial derivatives of the multiplication function wrt. each of the inputs:\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "\n",
    "#you do not need to do the partial derivative of the bias, because it isnt being multiplied.\n",
    "print('partial derivative of the multiplication function wrt x0: ', dmul_dx0)\n",
    "print('partial derivative of the multiplication function wrt x1: ', dmul_dx1)\n",
    "print('partial derivative of the multiplication function wrt x2: ', dmul_dx2, '\\n')\n",
    "\n",
    "\n",
    "#now, do chain rule of these derivatives with the subsequent one we did previously\n",
    "drelu_dx0 = drelu_dwx0 * dmul_dx0\n",
    "drelu_dx1 = drelu_dwx1 * dmul_dx1\n",
    "drelu_dx2 = drelu_dwx2 * dmul_dx2\n",
    "\n",
    "#partial derivatives of the multiplication function wrt. each of the weights:\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "\n",
    "print('partial derivative of the multiplication function wrt w0: ', dmul_dw0)\n",
    "print('partial derivative of the multiplication function wrt w1: ', dmul_dw1)\n",
    "print('partial derivative of the multiplication function wrt x2: ', dmul_dx2, '\\n')\n",
    "\n",
    "#now, do chain rule of these derivatives with subsequent one we did preiously\n",
    "drelu_dw0 = drelu_dwx0 * dmul_dw0\n",
    "drelu_dw1 = drelu_dwx0 * dmul_dw1\n",
    "drelu_dw2 = drelu_dwx0 * dmul_dw2\n",
    "\n",
    "print('partial derivative of the reLU function with resepct to the input 0: ', drelu_dx0)\n",
    "print('partial derivative of the reLU function with resepct to the weight 0: ', drelu_dw0)\n",
    "print('partial derivative of the reLU function with resepct to the input 1: ', drelu_dx1)\n",
    "print('partial derivative of the reLU function with resepct to the weight 1: ', drelu_dw1)\n",
    "print('partial derivative of the reLU function with resepct to the input 2: ', drelu_dx2)\n",
    "print('partial derivative of the reLU function with resepct to the weight 2: ', drelu_dw2)\n",
    "\n",
    "print('partial derivatve of the reLU function with respect to the bias: ', drelu_db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we will uses these partial derivatives, (which is the gradient), to the weights to minimize the output. this is referred to as the optimizer\n",
    "\n",
    "in this case, we will apply a negative fraction to the gradients of the weights, since we want to decrease the final output value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets show the current values of the weights and the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let now modify the weight values using the gradients of each respective weights:\n",
    "\n",
    "> changed the weights and biases slightly as to decrease the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0029999999999997, -0.994, 1.9910000000000003] 0.998\n"
     ]
    }
   ],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db\n",
    "\n",
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets do another forward pass and see the changes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0029999999999997, 1.988, 5.973000000000001]\n"
     ]
    }
   ],
   "source": [
    "#multiplying the inputs and weights\n",
    "input_weights = [x*w for x, w in zip(x, w)]\n",
    "print(input_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output before the reLU activation function:  5.956000000000001\n",
      "output after the reLU activation function:  5.956000000000001\n"
     ]
    }
   ],
   "source": [
    "#adding (performing the dot product, adding the bias)\n",
    "z = sum(input_weights) + b\n",
    "print('output before the reLU activation function: ', z)\n",
    "\n",
    "#reLU activation function\n",
    "y = max(z, 0)\n",
    "print('output after the reLU activation function: ', y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above code, we have minimized the reLU output. In a real world application, we do not minimize this layer, but rather the loss function. Remember we only did this for the reLU for demonstation purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop with multiple neurons\n",
    "\n",
    "now, instead of a single neuron, we have a layer with multiple neurons. During backprop, wach neuron from the current layer will recieve a vector of partial deriviatives instead of a sinlge value. \n",
    "\n",
    "below is the code to show this: \n",
    "\n",
    "> take the transposed weights, which are the transposed aray of the derivatives wrt the inputs, and multiply them by their respective gradients (related to the given neurons) to apply the chain rule.\n",
    "\n",
    "> then, we sum along with the inputs.\n",
    "\n",
    "> calculate the gradient for the next layer in the backpropogation. the next layer is the previous layer in the order of creation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients from the next layer:  [1. 1. 1.]\n",
      "dvalue shape:  (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#passed in gradient from the next layer. use vector of 1s for this example\n",
    "#remember, this will have to be a 2D array of rows and columns.\n",
    "dvalues = np.array([1., 1., 1.])\n",
    "print('gradients from the next layer: ', dvalues)\n",
    "print('dvalue shape: ', dvalues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the weights: \n",
      " [[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  10.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]]\n",
      "this is the weights shape (3, 4)\n",
      "this is the weights transposed: \n",
      " [[ 0.2   0.5  -0.26]\n",
      " [ 0.8  10.91 -0.27]\n",
      " [-0.5   0.26  0.17]\n",
      " [ 1.   -0.5   0.87]]\n",
      "this is now the weights shape:  (4, 3)\n"
     ]
    }
   ],
   "source": [
    "#we have 3 sets of weights, one set for each neuron. 4 inputs, thus 4 weights.\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], [0.5, 10.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]])\n",
    "print('this is the weights: \\n', weights)\n",
    "print('this is the weight''s shape', weights.shape)\n",
    "weights_T = weights.T\n",
    "print('this is the weights transposed: \\n', weights_T)\n",
    "print('this is now the weight''s shape: ', weights_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 11.44 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "#sum weights of the given input. multiply the passed in gradient for this neuron\n",
    "dx0 = sum(weights_T[0]) * dvalues[0]\n",
    "dx1 = sum(weights_T[1]) * dvalues[0]\n",
    "dx2 = sum(weights_T[2]) * dvalues[0]\n",
    "dx3 = sum(weights_T[3]) * dvalues[0]\n",
    "\n",
    "#gradient of the neuron's function wrt the inputs\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dinputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sum of the multiplicaiton of the elements is the dot product. we can achieve same by doing np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[ 0.44 11.44 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([1,1,1])\n",
    "print(dvalues.shape)\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], [0.5, 10.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "#sum weights of the given inputs, multiply the passed in each gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights)\n",
    "print(dinputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, lets account for batches of samples. Above, we are using a single sample responsible for a single gradient that is backpropagated between alyers. The row vector that we created for dvalues is in proparation for the batch of data.\n",
    "\n",
    "with more samples, the layer will return a list of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "[[ 0.44 11.44 -0.07  1.37]\n",
      " [ 0.88 22.88 -0.14  2.74]\n",
      " [ 1.32 34.32 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "print(dvalues.shape)\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], [0.5, 10.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "dinputs = np.dot(dvalues, weights)\n",
    "print(dinputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets combine the forward and backward pass of a single neuron with a full layer and batched-based partial derivatives. We'll minimize ReLU's output, once again, for this example only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  (3, 4)\n",
      "weights shape:  (4, 3)\n",
      "layer outputs before reLU activation function: \n",
      " [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "layer outputs after the reLU activation function: \n",
      " [[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]]\n",
      "[[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]]\n",
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#passed in gradient from the next layer, array of incremental gradient values\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "\n",
    "#we have 3 sets of inputs - samples\n",
    "inputs =  np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "print('input shape: ', inputs.shape)\n",
    "\n",
    "'''we have 3 sets of weights - one set for each neuron,\n",
    "   we have 4 inputs, thus 4 weights for each neuron'''\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "print('weights shape: ', weights.shape)\n",
    "\n",
    "#one bias for each neuron, biases are just a row vector of the shape (1, num_neurons)\n",
    "biases = np.array([[2,3,0.5]])\n",
    "\n",
    "#forward pass: perform the dot product, then perform the reLU activation function\n",
    "layer_outputs = np.dot(inputs, weights) + biases\n",
    "print('layer outputs before reLU activation function: \\n', layer_outputs)\n",
    "relu_outputs = np.maximum(0, layer_outputs)\n",
    "print('layer outputs after the reLU activation function: \\n', relu_outputs)\n",
    "\n",
    "'''lets optimize and test the backpropagation.\n",
    "    ReLU activation simulates derivative wrt the input values from the next layer\n",
    "    passed to the current layer during backpropagation\n",
    "'''\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "print(drelu)\n",
    "\n",
    "#Dense layer: \n",
    "#dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "#dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "#dbiases - sum values, do this over the samples (first axis), keepdims.\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "#update parameters\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we will update the dense layer and ReLU activation code with a backward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current weights: [[ 0.2   0.5  -0.26]\n",
      " [ 0.8  -0.91 -0.27]\n",
      " [-0.5   0.26  0.17]\n",
      " [ 1.   -0.5   0.87]]\n",
      "current biases: [[2.  3.  0.5]]\n",
      "current sample(s): [[ 1.   2.   3.   2.5]\n",
      " [ 2.   5.  -1.   2. ]\n",
      " [-1.5  2.7  3.3 -0.8]]\n",
      "output before reLU activation function: [[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "output after reLU activation function: [[4.8   1.21  2.385]\n",
      " [8.9   0.    0.2  ]\n",
      " [1.41  1.051 0.026]]\n",
      "current weights: [[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n",
      "current bias: [[6 6 6]]\n",
      "current inputs: [[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, num_features, num_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(num_features, num_neurons)\n",
    "        self.biases = np.zeros((1, num_neurons))\n",
    "\n",
    "    #remember, we need to remember what the inputs were, when doing backpropagation\n",
    "    def forward(self, samples):\n",
    "        self.outputs = np.dot(samples, self.weights) + self.biases\n",
    "        self.samples = samples\n",
    "\n",
    "    #it takes in the gradients from the next layer\n",
    "    def backward(self, dvalues):\n",
    "        #gradients on parameters\n",
    "        self.dweights = np.dot(self.samples.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        #gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class reLU:\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #since we need to modify the original variable, lets make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        #zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "X  = np.array([[1, 2, 3, 2.5],\n",
    "                [2., 5., -1., 2],\n",
    "                [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dvalues = np.array([[1,1,1],[2,2,2],[3,3,3]])\n",
    "\n",
    "dense1 = Dense_Layer(3, 3)\n",
    "\n",
    "dense1.weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "dense1.biases = np.array([[2,3,0.5]])\n",
    "\n",
    "print(f'current weights: {dense1.weights}\\ncurrent biases: {dense1.biases}')\n",
    "dense1.forward(X)\n",
    "print(f'current sample(s): {dense1.samples}')\n",
    "print(f'output before reLU activation function: {dense1.outputs}')\n",
    "activation1 = reLU()\n",
    "activation1.forward(dense1.outputs)\n",
    "print(f'output after reLU activation function: {activation1.outputs}')\n",
    "\n",
    "dense1.backward(dvalues)\n",
    "print(f'current weights: {dense1.dweights}\\ncurrent bias: {dense1.dbiases}\\ncurrent inputs: {dense1.dinputs}')\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
