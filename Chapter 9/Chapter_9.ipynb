{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "by calculating the gradients, which is the vector of all possible partial derivatives of a functiion with respect to all of its respective parameters, we can do the chain rule, which is the derivative of the chained functions, which are the product of the partial derivatives with respect to the loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets demonstrate the backpropagation by computing it at the ReLU activation output layer first. We will then do this for the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#the input values, its weights, and the bias\n",
    "x = [1.0, -2.0, 3.0]\n",
    "w = [-3.0, -1.0, 2.0]\n",
    "b = 1.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above shows 2 vectors: the inputs and weights, and the bias.\n",
    "\n",
    "as you can see, the number of neurons for the subsequent layer will be 1, given that there is only one sample for x, and one set of weights for the inputs.\n",
    "\n",
    "(n_samples, num_features) * (num_features, n_neurons) = (n_samples, n_neurons)\n",
    "\n",
    "(1, 3) * (3, 1) = (1, 1), so single value for the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, 2.0, 6.0]\n"
     ]
    }
   ],
   "source": [
    "input_weights = [x*w for x, w in zip(x, w)]\n",
    "print(input_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_weights represent the input and weight muliplication operation, multiplying each element in x with each element in w element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "z = sum(input_weights) + b\n",
    "print(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z represents the sum of input_weights, + the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, we compute the reLU activation function, which returns z is z > 0, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
