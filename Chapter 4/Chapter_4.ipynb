{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Activation functions are applied to the output of the neurons (or layer of neurons), which modifies the outputs. \n",
    "\n",
    "It makes the output non-linear:\n",
    "\n",
    "> allows neural networks with usually two or more hidden layers to map nonlinear functions.\n",
    "\n",
    "> when non-linear, NNs can approximate functions that ARE non-linear.\n",
    "\n",
    "In general, your neural network will have two types of activation functions: \n",
    "\n",
    "* activation functions used for the hidden neurons\n",
    "\n",
    "* activation functions used for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "'''ReLU Activation Function Code'''\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "'''if the number in the input is greater than 0, append the input to the output list\n",
    ", otherwise, append 0 to the output list.'''\n",
    "outputs = []\n",
    "for i in inputs:\n",
    "    if i > 0:\n",
    "        outputs.append(i)\n",
    "    else:\n",
    "        outputs.append(0)   \n",
    "\n",
    "print(outputs)         "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternatively, we can take take the largest of the two values: 0 or the neuron value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "\n",
    "outputs = []\n",
    "\n",
    "'''for each number in the input, compare the input to 0, it takes the largest of the two\n",
    "values (if i = -1, take 0, if i = 100, take 100.'''\n",
    "for i in inputs:\n",
    "    outputs.append(max(0, i))\n",
    "\n",
    "print(outputs)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy contains an equivalent: np.maximum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "outputs = np.maximum(0, inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a new Rectified Linear Activation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "#Creation of the Dense Layer Class\n",
    "class Dense_Layer:\n",
    "    def __init__(self, num_features, num_neurons):\n",
    "        self.weights = np.random.randn(num_features, num_neurons)\n",
    "        self.biases = np.zeros((1, num_neurons))\n",
    "\n",
    "    def forward(self, samples):\n",
    "        self.outputs = np.dot(samples, self.weights) + self.biases        \n",
    "\n",
    "#ReLU Activation Class\n",
    "class Activation_ReLU:\n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        #Calculate the output values from the inputs\n",
    "        self.outputs = np.maximum(0, inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now apply this activation function to the dense layer's outputs in our code \n",
    "from the last chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 samples in this dataset: \n",
      " [[ 0.          0.        ]\n",
      " [ 0.0064161   0.00780154]\n",
      " [-0.00603296  0.01928017]\n",
      " [ 0.00582981  0.02973696]\n",
      " [ 0.00544852  0.04003499]] \n",
      "\n",
      "The datasets shape:  (300, 2)\n",
      "Reminder that 300 indicates the number of samples, 2 being the features.\n",
      "This means that if you want to connect this to 3 neurons, weights shape is (2,3)\n",
      "[[0.         0.         0.        ]\n",
      " [0.         0.01061704 0.01824998]\n",
      " [0.00161633 0.         0.01487057]\n",
      " [0.         0.01684239 0.04383836]\n",
      " [0.         0.01963051 0.05570489]]\n"
     ]
    }
   ],
   "source": [
    "#Creeation of the dataset using spiral data from nnfs\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "#Lets analyze this data:\n",
    "print('First 5 samples in this dataset: \\n', X[:5], '\\n')\n",
    "print('The dataset''s shape: ', X.shape)\n",
    "print('Reminder that 300 indicates the number of samples, 2 being the features.')\n",
    "print('This means that if you want to connect this to 3 neurons, weights shape is (2,3)')\n",
    "\n",
    "\n",
    "#Creation of an instance of the Dense Layer Class, with 2 input features and 3 output values (neurons)\n",
    "dense1 = Dense_Layer(2, 3)\n",
    "\n",
    "#Creation of an instance of the ReLU Activation Class\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "#Make a forwardf pass of our training data through this data\n",
    "dense1.forward(X)\n",
    "\n",
    "#Forward pass through the activation function. Takes in output from the previous layer\n",
    "activation1.forward(dense1.outputs)\n",
    "\n",
    "#Show the outputs of the frist few samples\n",
    "print(activation1.outputs[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show the softmax activation function in play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first step is to exponentiate the outputs, using Euler's number, e. (2.7182818...)\n",
    "\n",
    "> this is also referred to as the exponential growth of a number.\n",
    "\n",
    "> exponentiating is taking this constant e to the power of a given parameter:\n",
    "\n",
    "$$ y = e^{x} $$\n",
    "\n",
    "the softmax activation function:\n",
    "\n",
    "$$ \\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}} $$\n",
    "\n",
    "where z is the given the given indices, i is the current sample in z and j is the outut in i.\n",
    "the numberatir exponentiates the current output value and the denominator takes a sum of all the exponentiated outputs for a given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values:  [121.51041751893969, 3.3534846525504487, 10.85906266492961]\n"
     ]
    }
   ],
   "source": [
    "#Values from the previous output when we described what a neural network is.\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "#e, the exponential constant, denoted as E. You can also used math.e, this is an approx.\n",
    "E = 2.71828182846\n",
    "\n",
    "#for each value in a vector, calculate the exponential value (E ^ output)\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E ** output)\n",
    "print('Exponentiated values: ', exp_values)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can simplify this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values:  [121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "exp_values = [(math.e) ** output for output in layer_outputs]\n",
    "print('Exponentiated values: ', exp_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the exponential value of any number is always non-negative, because a negative probability doesnt make sense. \n",
    "\n",
    "> it returns 0 for negative infinity, 1 for the input of 0, and increases for positive values\n",
    "\n",
    "exponential function is monotonic, which means that the higher the input values, the outputs will also be higher naturally. It also provides stability to the result as a normalized exponentiation is more about the difference between the numbers than the magnitudes of those numbers.\n",
    "\n",
    "once exponentiated, we convert these values to a probability distribution (vector of confidences), one for each class, which adds up to 1 for everthing in the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The normalized exponentiated values:\n",
      " [0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of the normalized values:  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "#lets normalize the values\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print('The normalized exponentiated values:\\n', norm_values)  \n",
    "print('Sum of the normalized values: ', sum(norm_values))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets perform this same set of operations with the use of NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponentiated values \n",
      " [121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "Normalized values: \n",
      " [0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "Sum of these normalized values:  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "#for each value in a vector, calculate the exponential values:\n",
    "exp_values = [math.e ** output for output in layer_outputs]\n",
    "print('Exponentiated values \\n', exp_values)\n",
    "\n",
    "#lets now normalize the data to create a vector of confidences (probability)\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = [value / norm_base for value in exp_values]\n",
    "print('Normalized values: \\n', norm_values)\n",
    "print('Sum of these normalized values: ', sum(norm_values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify even further, using np.exp(), then immediately normalizing them with the sum. We can do this to train in batch more effiecienty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites: \n",
      " [0.89528266 0.02470831 0.08000903] Shape:  (3,)\n"
     ]
    }
   ],
   "source": [
    "#get exponentiated probabilities\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    "#normalize them for each of the sample\n",
    "probabilities = exp_values / np.sum(exp_values, keepdims=True)\n",
    "print('Probabilites: \\n', probabilities, 'Shape: ', probabilities.shape )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy, axis refers to a specific dimension of an array.\n",
    "\n",
    "for example: a 2D array has 2 axis:\n",
    "\n",
    "* row axis, axis=0, column axis, axis= 1\n",
    "\n",
    "for a 3D array, it has 3 axis:\n",
    "\n",
    "* depth axis, axis=0, row axis, axis=1, and column axis, axis=1\n",
    "\n",
    "When performing operations on the arrays, axis is often usef to specify along which axis the operation should be performed.\n",
    "\n",
    "For example: if axis=0, np.sum() will sum along the rows, axis=1 for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the array:  (3, 3)\n",
      "Sum without axis:  18.172\n",
      "This will be identical to the above since defauly is None,  18.172\n",
      "Another way to think of it w/ a matrix == axis 0: columns: \n",
      "[15.11   0.451  2.611]\n"
     ]
    }
   ],
   "source": [
    "'''Lets demonstrate this axis parameter'''\n",
    "import numpy as np\n",
    "\n",
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                [8.9, -1.81, 0.2],\n",
    "                [1.41, 1.051, 0.026]])\n",
    "\n",
    "print('The shape of the array: ', layer_outputs.shape)\n",
    "\n",
    "print('Sum without axis: ', np.sum(layer_outputs))\n",
    "\n",
    "print('This will be identical to the above since defauly is None, ', np.sum(layer_outputs, axis=None))\n",
    "\n",
    "print('Another way to think of it w/ a matrix == axis 0: columns: ')\n",
    "print(np.sum(layer_outputs, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this isn't what we want though, we want the sum of the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.395\n",
      "7.29\n",
      "2.4869999999999997\n"
     ]
    }
   ],
   "source": [
    "'''From scratch version'''\n",
    "for i in layer_outputs:\n",
    "    print(sum(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So we can sum axis 1, but not the current shape:  [8.395 7.29  2.487]\n",
      "this shape is:  (3,)\n"
     ]
    }
   ],
   "source": [
    "print('So we can sum axis 1, but not the current shape: ', np.sum(layer_outputs, axis=1))\n",
    "print(\"this shape is: \", np.sum(layer_outputs, axis=1).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sum at axis 1, but keep the same dimensions as the layer_outputs (which is (3, 3)) so that we want to sum to either be (1, n), or (n, 1)\n",
    "\n",
    "In this case will represent it as a (n, 1) 2 Dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum axis = 1, but keep the same dimension as the input: \n",
      " [[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n",
      "the keepdims =  True keeps the dimensions the same as the input.\n"
     ]
    }
   ],
   "source": [
    "print('Sum axis = 1, but keep the same dimension as the input: \\n', np.sum(layer_outputs, axis=1, keepdims=True))\n",
    "print('the keepdims =  True keeps the dimensions the same as the input.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we divide the array containing a batch of outputs with the array, NumPy will perform this spample-wise. \n",
    "\n",
    "> This means that it will divide all of the values from each of the output rows by the corresponding row from the sum array. \n",
    "\n",
    "> since the sum in each row is a single value, it will be used for the division with every value from the coresponding output row."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now combine all of we have went over thus far into the softmax class we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax activation\n",
    "class Activation_SoftMax:\n",
    "    '''define the forward pass:\n",
    "            get the exponentiated values for each of the inputs\n",
    "            normalize each of the exponentiated values\n",
    "\n",
    "        we subtract the maximum value of each input row from each element of\n",
    "        the output, then exponentiating the result. This is so that the \n",
    "        exponentiated values do not become too large and cause numerical values\n",
    "    '''\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we subtracted the largest of the inputs before we did the exponentiation.\n",
    "\n",
    "this solves 2 main challenges: dead neurons and very large numbers (exploding values)\n",
    "\n",
    "> the exponential function used in softmax activation function is one of the sources of exploding values.\n",
    "\n",
    "Below is an example of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n",
      "22026.465794806718\n",
      "inf\n",
      "This shows that the exp function tends toward 0 as the input value\n",
      "apporaches negative inf, 1 when input is 0\n",
      "0.0 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/b0xs89p96wg8lzcfm69hdsm40000gn/T/ipykernel_1559/3490478942.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(100000))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.exp(1))\n",
    "print(np.exp(10))\n",
    "print(np.exp(100000))\n",
    "\n",
    "print('This shows that the exp function tends toward 0 as the input value\\napporaches negative inf, 1 when input is 0')\n",
    "print(np.exp(-np.inf), np.exp(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we subtract the maximum value from the list of inputs values, the output values will be in a range from the negative value up to 0, as the largest number is subtractef ny itself is 0, and only smaller number by it will result in a negative number.  \n",
    "\n",
    "Thanks to normalization, we can subtract any value from all of the inputs, and it will not change the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_SoftMax()\n",
    "\n",
    "softmax.forward([[1,2,3]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "'''We can subtract each of the values in the input by th largest value in \n",
    "the input, and it will not change the probability output, because it is\n",
    "normalized!'''\n",
    "\n",
    "softmax.forward([[-2,-1,0]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets add another dense layer as the output layer, setting it to contain as many inputs as the previous layer has outputs and as many outputs as our data includes classes. We can then apply softmax function to the output of this new layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33244723 0.33378834 0.33376444]\n",
      " [0.33474389 0.33177042 0.3334857 ]\n",
      " [0.34113263 0.32474825 0.33411912]\n",
      " [0.3302289  0.33492762 0.33484348]]\n"
     ]
    }
   ],
   "source": [
    "#create the dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "#create a dense layer with two input features, and 3 output values (neurons)\n",
    "dense1 = Dense_Layer(2, 3)\n",
    "\n",
    "#create an ReLU activation function (to be used with the dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "'''create a second dense layer with 3 input features (the 3 neurons), \n",
    "   and 3 output values (3 succeeding neurons)'''\n",
    "dense2 = Dense_Layer(3, 3)\n",
    "\n",
    "#create a softmax activation function (to be used with the dense layer)\n",
    "activation2 = Activation_SoftMax()\n",
    "\n",
    "#make a forward pass of out training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "#pass the output layer in the first dense layer to the ReLu activation function\n",
    "activation1.forward(dense1.outputs)\n",
    "\n",
    "#pass the result from the ReLu activation function into the second dense layer\n",
    "dense2.forward(activation1.outputs)\n",
    "\n",
    "#pass the result from the second dense layer into the softmax function\n",
    "activation2.forward(dense2.outputs)\n",
    "\n",
    "#print the output from the second dense layer\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the distribution of predictions is almost equal, as each of the samples has ~33% predictions for each class. this output is the confidence scores for each of the samples.\n",
    "\n",
    "To determine which classification the model ahs chosen to be the prediction, we perform the argmax on these outputs, which checks which of the classes in the ouput distribution has the highest confidence and returns its index.\n",
    "\n",
    "Example:\n",
    "\n",
    "> argmax of [0.22, 0.6, 0.18] is the same as argmax for [0.32, 0.36, 0.32], value returned will both be 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code of this Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "#Create the Dense Layer Class\n",
    "class Dense_Layer:\n",
    "    #initialize the weights and biases for the layer\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_features, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    #perform the forward pass, which is the matrix product of inputs, weights, add biases.\n",
    "    def forward(self, samples):\n",
    "        self.outputs = np.dot(samples, self.weights) + self.biases\n",
    "\n",
    "#Create the ReLU Activation Function\n",
    "class ReLU:\n",
    "    '''create the forward pass, x if x > 0, otherwise 0\n",
    "       np.maximum returns input if input > 0, if less than 0, return 0'''\n",
    "    def forward(self, inputs):\n",
    "        self.outputs = np.maximum(0, inputs) \n",
    "\n",
    "#Create the SoftMax Activation Function\n",
    "class SoftMax:\n",
    "    '''create the forward pass, compute the exponentiated values,\n",
    "       normalize them for each sample'''      \n",
    "    def forward(self, inputs):\n",
    "        #get the unnormalized probablities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "         # normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.outputs = probabilities\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Dense_Layer(2, 3)\n",
    "activation1 = ReLU()\n",
    "dense2 = Dense_Layer(3, 3)\n",
    "activation2 = SoftMax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.outputs)\n",
    "dense2.forward(activation1.outputs)\n",
    "activation2.forward(dense2.outputs)\n",
    "\n",
    "print(activation2.outputs[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:14) \n[Clang 12.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
